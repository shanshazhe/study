from pathlib import Path
from typing import Tuple, Union

import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    pipeline,
)
from transformers.pipelines import SUPPORTED_TASKS

MODEL_ID = "distilbert-base-uncased-finetuned-sst-2-english"
LOCAL_MODEL_DIR = Path("saved_models") / MODEL_ID.replace("/", "_")


def get_device() -> Union[torch.device, int]:
    """
    Choose the best available device for inference.
    - CUDA takes priority when present.
    - Fall back to MPS on Apple Silicon.
    - Otherwise default to CPU.
    """
    if torch.cuda.is_available():
        return 0
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def list_supported_tasks() -> None:
    print(f"Found {len(SUPPORTED_TASKS)} pipeline tasks:")
    for task in sorted(SUPPORTED_TASKS.keys()):
        print(f"- {task}")


def load_or_download_model() -> Tuple[AutoTokenizer, AutoModelForSequenceClassification, Path]:
    """
    Download the model/tokenizer once, save locally, and load from disk afterward.
    """
    LOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)

    if any(LOCAL_MODEL_DIR.iterdir()):
        print(f"Loading cached model from {LOCAL_MODEL_DIR}")
        tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
        model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
    else:
        print(f"Downloading {MODEL_ID} and saving to {LOCAL_MODEL_DIR}")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)
        tokenizer.save_pretrained(LOCAL_MODEL_DIR)
        model.save_pretrained(LOCAL_MODEL_DIR)

    return tokenizer, model, LOCAL_MODEL_DIR


def run_sentiment_pipeline() -> None:
    """Lightweight end-to-end pipeline example."""
    device = get_device()
    tokenizer, model, model_dir = load_or_download_model()
    classifier = pipeline(
        "sentiment-analysis",
        model=model_dir,
        tokenizer=tokenizer,
        device=device,
    )
    texts = [
        "Hugging Face pipelines are convenient.",
        "Tokenization can be confusing at first.",
    ]
    print("\nPipeline results:")
    for text, result in zip(texts, classifier(texts)):
        print(f"'{text}' -> {result}")


def run_tokenizer_only() -> None:
    """Manual tokenizer + model usage without the high-level pipeline."""
    tokenizer, model, _ = load_or_download_model()

    batch = tokenizer(
        ["I love good documentation.", "The error messages are unclear."],
        padding=True,
        truncation=True,
        return_tensors="pt",
    )

    with torch.no_grad():
        outputs = model(**batch)
        scores = outputs.logits.softmax(dim=-1)

    print(f"Tokenizer results:")
    label_names = model.config.id2label
    print(f"Label names:")
    print(label_names)
    print("\nTokenizer + model results:")
    for i, input_ids in enumerate(batch["input_ids"]):
        text = tokenizer.decode(input_ids, skip_special_tokens=True)
        label_scores = {label_names[j]: float(scores[i, j]) for j in range(scores.size(1))}
        print(f"'{text}' -> {label_scores}")


    print(scores.size(0))
    for i in range(scores.size(0)):
        print("predicted label for sample", i, ":", label_names[torch.argmax(scores[i]).item()])


def run_tokenizer_encode_example() -> None:
    """Simple example using tokenizer.encode() to understand tokenization."""
    tokenizer, _, _ = load_or_download_model()

    # Sample text to encode
    sample_text = "This sample helps me study and understand tokenization."

    print("\n=== Tokenizer Encode Example ===")
    print(f"Original text: '{sample_text}'")

    # Method 1: encode() - returns token IDs as a list
    token_ids = tokenizer.encode(sample_text)
    print(f"\nToken IDs (encode): {token_ids}")

    # Method 2: encode() with add_special_tokens=False
    token_ids_no_special = tokenizer.encode(sample_text, add_special_tokens=False)
    print(f"Token IDs without special tokens: {token_ids_no_special}")

    # Method 3: encode() with custom max_length and truncation
    # å¢å¤§max_lengthæ¥å…è®¸æ›´é•¿çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ä»é»˜è®¤çš„512å¢åŠ åˆ°1024
    token_ids_long = tokenizer.encode(
        sample_text,
        max_length=1024,  # å¢å¤§åˆ°1024ï¼Œé»˜è®¤é€šå¸¸æ˜¯512
        truncation=True,   # å¯ç”¨truncation
        add_special_tokens=True
    )
    print(f"\nToken IDs with max_length=1024: {token_ids_long}")

    # Decode back to text
    decoded_text = tokenizer.decode(token_ids)
    print(f"\nDecoded text: '{decoded_text}'")

    # Convert to tokens (subwords)
    tokens = tokenizer.convert_ids_to_tokens(token_ids)
    print(f"\nTokens: {tokens}")

    # Show each token with its ID
    print("\nToken-by-token breakdown:")
    for token, token_id in zip(tokens, token_ids):
        print(f"  '{token}' -> ID: {token_id}")

    # æ¼”ç¤ºä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬è¢«truncateçš„æƒ…å†µ
    long_text = " ".join(["This is a very long sentence."] * 50)
    print(f"\n=== Truncation Demo ===")
    print(f"Long text length: {len(long_text)} characters")

    # ä½¿ç”¨è¾ƒå°çš„max_length
    token_ids_short = tokenizer.encode(long_text, max_length=20, truncation=True)
    print(f"\nWith max_length=20: {len(token_ids_short)} tokens")
    print(f"Truncated text: '{tokenizer.decode(token_ids_short)}'")

    # ä½¿ç”¨è¾ƒå¤§çš„max_length
    token_ids_longer = tokenizer.encode(long_text, max_length=200, truncation=True)
    print(f"\nWith max_length=200: {len(token_ids_longer)} tokens")
    print(f"Less truncated text: '{tokenizer.decode(token_ids_longer)[:100]}...'")

    # ä½¿ç”¨æ›´å¤§çš„max_length
    token_ids_longest = tokenizer.encode(long_text, max_length=512, truncation=True)
    print(f"\nWith max_length=512: {len(token_ids_longest)} tokens")

    # ===== Padding ç¤ºä¾‹ =====
    print("\n=== Padding Demo ===")
    print("æ³¨æ„ï¼šencode() æ–¹æ³•ä¸æ”¯æŒpaddingï¼éœ€è¦ä½¿ç”¨ tokenizer() æˆ– encode_plus()")

    # ä¸¤ä¸ªä¸åŒé•¿åº¦çš„æ–‡æœ¬
    short_text = "Hello"
    medium_text = "This is a longer sentence with more words."

    # æ–¹æ³•1: ä½¿ç”¨ encode() - ä¸ä¼špadding
    print("\nä½¿ç”¨ encode() - æ²¡æœ‰padding:")
    ids_short = tokenizer.encode(short_text, max_length=20, truncation=True)
    ids_medium = tokenizer.encode(medium_text, max_length=20, truncation=True)
    print(f"çŸ­æ–‡æœ¬ '{short_text}': {ids_short} (é•¿åº¦: {len(ids_short)})")
    print(f"ä¸­æ–‡æœ¬: {ids_medium} (é•¿åº¦: {len(ids_medium)})")
    print("ğŸ‘† æ³¨æ„ï¼šä¸¤ä¸ªæ–‡æœ¬é•¿åº¦ä¸åŒï¼Œæ— æ³•ç»„æˆbatchï¼")

    # æ–¹æ³•2: ä½¿ç”¨ tokenizer() - å¯ä»¥padding
    print("\nä½¿ç”¨ tokenizer() - æœ‰padding:")
    batch_result = tokenizer(
        [short_text, medium_text],
        max_length=20,
        padding='max_length',  # å¡«å……åˆ°max_length
        truncation=True,
        return_tensors='pt'  # è¿”å›PyTorch tensor
    )
    print(f"input_ids shape: {batch_result['input_ids'].shape}")
    print(f"çŸ­æ–‡æœ¬ token IDs: {batch_result['input_ids'][0].tolist()}")
    print(f"ä¸­æ–‡æœ¬ token IDs: {batch_result['input_ids'][1].tolist()}")
    print(f"attention_mask: \n{batch_result['attention_mask']}")
    print(f"ğŸ‘† æ³¨æ„ï¼šçŸ­æ–‡æœ¬ç”¨ {tokenizer.pad_token_id} (PAD token) å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼")

    # æ–¹æ³•3: ä½¿ç”¨ encode_plus() - å•ä¸ªæ–‡æœ¬padding
    print("\nä½¿ç”¨ encode_plus() - å•ä¸ªæ–‡æœ¬padding:")
    encoded = tokenizer.encode_plus(
        short_text,
        max_length=15,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    print(f"çŸ­æ–‡æœ¬ '{short_text}' paddingåˆ°15:")
    print(f"input_ids: {encoded['input_ids'][0].tolist()}")
    print(f"attention_mask: {encoded['attention_mask'][0].tolist()}")

    # å±•ç¤ºä¸åŒçš„paddingç­–ç•¥
    print("\nä¸åŒçš„paddingç­–ç•¥:")
    texts = ["Hi", "Hello world", "This is a test"]

    # longest: paddingåˆ°batchä¸­æœ€é•¿çš„åºåˆ—
    batch_longest = tokenizer(texts, padding='longest', return_tensors='pt')
    print(f"\npadding='longest': shape {batch_longest['input_ids'].shape}")
    for i, text in enumerate(texts):
        print(f"  '{text}': {batch_longest['input_ids'][i].tolist()}")

    # max_length: paddingåˆ°æŒ‡å®šé•¿åº¦
    batch_max = tokenizer(texts, padding='max_length', max_length=10, truncation=True, return_tensors='pt')
    print(f"\npadding='max_length' (10): shape {batch_max['input_ids'].shape}")
    for i, text in enumerate(texts):
        print(f"  '{text}': {batch_max['input_ids'][i].tolist()}")


def compare_auto_vs_manual_padding() -> None:
    """
    å¯¹æ¯” pipeline è‡ªåŠ¨ padding å’Œæ‰‹åŠ¨ tokenizer çš„è¡Œä¸º
    """
    tokenizer, model, model_dir = load_or_download_model()
    device = get_device()

    # æµ‹è¯•æ–‡æœ¬ï¼šé•¿åº¦æ˜æ˜¾ä¸åŒ
    texts = [
        "Good",  # å¾ˆçŸ­
        "This is a longer sentence with more words",  # è¾ƒé•¿
    ]

    print("\n" + "="*60)
    print("å¯¹æ¯”ï¼šPipeline è‡ªåŠ¨å¤„ç† vs æ‰‹åŠ¨ Tokenizer")
    print("="*60)

    # ===== 1. Pipeline è‡ªåŠ¨å¤„ç† =====
    print("\nã€æ–¹å¼1ï¼šä½¿ç”¨ pipeline() - è‡ªåŠ¨paddingã€‘")
    classifier = pipeline(
        "sentiment-analysis",
        model=model_dir,
        tokenizer=tokenizer,
        device=device,
    )
    results = classifier(texts)
    print(f"âœ… Pipeline è‡ªåŠ¨å¤„ç†äº†ä¸åŒé•¿åº¦çš„æ–‡æœ¬")
    for text, result in zip(texts, results):
        print(f"  '{text}' -> {result}")

    # ===== 2. æ‰‹åŠ¨ tokenizer WITHOUT padding =====
    print("\nã€æ–¹å¼2ï¼šæ‰‹åŠ¨ tokenizer ä¸å¸¦ padding - ä¼šå¤±è´¥ã€‘")
    try:
        batch_no_padding = tokenizer(
            texts,
            truncation=True,
            return_tensors="pt",
            # æ³¨æ„ï¼šæ²¡æœ‰ padding=True
        )
        print(f"âŒ è¿™é€šå¸¸ä¼šå¤±è´¥ï¼Œå› ä¸ºé•¿åº¦ä¸ä¸€è‡´")
        print(f"   input_ids shape: {batch_no_padding['input_ids'].shape}")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼ˆé¢„æœŸï¼‰: {type(e).__name__}")
        print(f"   åŸå› ï¼šæ–‡æœ¬é•¿åº¦ä¸åŒï¼Œæ— æ³•ç»„æˆtensor batch")

    # ===== 3. æ‰‹åŠ¨ tokenizer WITH padding =====
    print("\nã€æ–¹å¼3ï¼šæ‰‹åŠ¨ tokenizer å¸¦ padding=Trueã€‘")
    batch_with_padding = tokenizer(
        texts,
        padding=True,  # å…³é”®å‚æ•°
        truncation=True,
        return_tensors="pt",
    )
    print(f"âœ… æˆåŠŸï¼input_ids shape: {batch_with_padding['input_ids'].shape}")
    print(f"\nPAD token ID: {tokenizer.pad_token_id}")
    print(f"PAD token: '{tokenizer.pad_token}'")

    for i, text in enumerate(texts):
        ids = batch_with_padding['input_ids'][i].tolist()
        mask = batch_with_padding['attention_mask'][i].tolist()
        print(f"\næ–‡æœ¬ {i+1}: '{text}'")
        print(f"  token_ids:      {ids}")
        print(f"  attention_mask: {mask}")
        print(f"  padding count:  {ids.count(tokenizer.pad_token_id)} ä¸ª")

    # ===== 4. Pipeline å†…éƒ¨å®é™…åšäº†ä»€ä¹ˆ =====
    print("\n" + "="*60)
    print("ğŸ” Pipeline å†…éƒ¨è‡ªåŠ¨æ‰§è¡Œçš„æ­¥éª¤ï¼š")
    print("="*60)
    print("1. è‡ªåŠ¨è°ƒç”¨ tokenizer(..., padding=True)")
    print("2. è‡ªåŠ¨å°† tensors ç§»åˆ°æ­£ç¡®çš„ device")
    print("3. è‡ªåŠ¨æ‰§è¡Œ model(**inputs)")
    print("4. è‡ªåŠ¨è¿›è¡Œ post-processing")
    print("\nğŸ‘‰ æ‰€ä»¥ä½¿ç”¨ pipeline æ—¶ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨ paddingï¼")


if __name__ == "__main__":
    # list_supported_tasks()
    # run_sentiment_pipeline()
    # run_tokenizer_only()
    # run_tokenizer_encode_example()
    compare_auto_vs_manual_padding()
